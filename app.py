import os
import pandas as pd
import chainlit as cl
import urllib.parse
from dotenv import load_dotenv
from chainlit.context import get_context
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_community.tools import TavilySearchResults
from langchain.schema import Document
from langchain_core.messages import HumanMessage
from urllib.parse import unquote, parse_qs
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware

# Load .env
load_dotenv()

# API Keys
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

dataset_memory = {}

# ì „ì—­ ê°ì²´ë“¤ ì„ ì–¸ (start í•¨ìˆ˜ ë°–ìœ¼ë¡œ ëºŒ)
llm = ChatOpenAI(model_name="gpt-4o-mini")
tavily_tool = TavilySearchResults(
    max_results=5,
    include_answer=True,
    include_raw_content=True,
    include_domains=["m.entertain.naver.com"]
)
summary_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ë§Œ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”."),
    ("human", "ë‹¤ìŒì€ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. {search_content}.ë‹¤ìŒì€ ìš”ì²­ì‚¬í•­ì…ë‹ˆë‹¤.{target}ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì²­ì‚¬í•­ê³¼ ê´€ë ¨ëœ ê²°ê³¼ë§Œ ê°€ì§€ê³  ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n")
])

def summarize_search(user_input: str) -> str:
    search_results = tavily_tool.invoke(user_input)
    if not search_results or not isinstance(search_results, list):
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    all_content = "\n\n".join([item.get("content", "") for item in search_results])
    prompt_text = summary_prompt.format(search_content=all_content,target=user_input)
    response = llm.invoke([HumanMessage(content=prompt_text)])
    return response.content.strip()

# ğŸ“Œ ë¶„ê¸° ë¡œì§ í¬í•¨í•œ process_docs í•¨ìˆ˜
def process_docs(file_path: str) -> tuple[list[Document], PromptTemplate]:
    filename = os.path.splitext(os.path.basename(file_path))[0]
    docs = []

    if "ë“±ì¥ì¸ë¬¼" in filename or "3ì›”ì´í›„" in filename:
        loader = CSVLoader(
            file_path=file_path,
            encoding="utf-8",
            metadata_columns=["character_name", "actor_name"]
        )
        raw_docs = loader.load()
        print("âœ… ë“±ì¥ì¸ë¬¼ or 3ì›”ì´í›„ CSV ë¡œë”© ì„±ê³µ")

        # ğŸ§  ì‘í’ˆëª… ìë™ ì¶”ì¶œ
        title = filename.replace("_ë“±ì¥ì¸ë¬¼", "").replace(" ë“±ì¥ì¸ë¬¼", "").replace("_", " ").strip()

        docs = []
        for doc in raw_docs:
            metadata = doc.metadata
            character_name = metadata.get("character_name")
            actor_name = metadata.get("actor_name")
            content = doc.page_content.strip() if doc.page_content else ""

            if character_name and actor_name:
                # ğŸ‘‡ descriptionì„ ì „ì²´ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                description = content.strip()

                new_content = f"""
            ë“œë¼ë§ˆ '{title}'ì— ë“±ì¥í•˜ëŠ” ì¸ë¬¼ '{character_name}'ì€(ëŠ”),
            ë°°ìš° '{actor_name}'ì´ ì—°ê¸°í•œ ì—­í• ì…ë‹ˆë‹¤.
            {description}
            """.strip()

                # âœ… ë¬¸ì„œ í™•ì¸ ë¡œê·¸ ì¶”ê°€
                print(f"ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ: {new_content}")

                docs.append(Document(
                    page_content=new_content,
                    metadata={
                        "character_name": character_name,
                        "actor_name": actor_name
                    }
                ))

                

        prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ ë¬¸ë§¥(Context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥ì— ê´€ë ¨ëœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¬¸ë§¥:
{context}

ë‹µë³€:
""")

    elif "naver_entertainment_articles" in filename:
        loader = CSVLoader(file_path=file_path, encoding="utf-8", metadata_columns=["title", "content"])
        raw_docs = loader.load()
        print("âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ CSV ë¡œë”© ì„±ê³µ")

        for doc in raw_docs:
            metadata = doc.metadata
            title = metadata.get("title")
            content = metadata.get("content")
            full_content = doc.page_content
            if title and content:
                description = full_content.split(",")[-1].strip()
                new_content = f"{title}ì€(ëŠ”) {content}ì¸ ë‚´ìš©ì…ë‹ˆë‹¤. {description}"
                docs.append(Document(page_content=new_content, metadata={"title": title}))

        prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ ë¬¸ë§¥(Context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥ì— ê´€ë ¨ëœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¬¸ë§¥:
{context}

ë‹µë³€:
""")

    elif "kinolights_ranking_data" in filename:
        loader = CSVLoader(file_path=file_path, encoding="utf-8", metadata_columns=[
            'ì œëª©', 'ì¢…ë¥˜', 'ì¥ë¥´', 'ë°©ì˜ì¼', 'ê°œë´‰ì¼',
            'ì±„ë„', 'ì œì‘êµ­ê°€', 'ì œì‘ì—°ë„', 'ì—°ë ¹ë“±ê¸‰',
            'ì˜¤ë¦¬ì§€ë„', 'ë°°ìš°', 'ëŸ¬ë‹íƒ€ì„', 'URL'
        ])
        raw_docs = loader.load()
        print("âœ… Kinolights CSV ë¡œë”© ì„±ê³µ")

        for doc in raw_docs:
            metadata = doc.metadata
            title = metadata.get("ì œëª©")
            genre = metadata.get("ì¥ë¥´")
            content_type = metadata.get("ì¢…ë¥˜")
            country = metadata.get("ì œì‘êµ­ê°€")
            production_year = metadata.get("ì œì‘ì—°ë„")
            release_date = metadata.get("ê°œë´‰ì¼")
            broadcast_date = metadata.get("ë°©ì˜ì¼")
            channel = metadata.get("ì±„ë„")
            age_rating = metadata.get("ì—°ë ¹ë“±ê¸‰")
            is_original = metadata.get("ì˜¤ë¦¬ì§€ë„")
            actors = metadata.get("ë°°ìš°")
            running_time = metadata.get("ëŸ¬ë‹íƒ€ì„")
            url = metadata.get("URL")
            content = doc.page_content
            description = content.split(",")[-1].strip()

            new_content = f"""
'{title}'ì€(ëŠ”) {production_year}ë…„ì— {country}ì—ì„œ ì œì‘ëœ {genre} {content_type}ì…ë‹ˆë‹¤.
ì£¼ìš” ì¶œì—°ì§„ìœ¼ë¡œëŠ” {actors}ê°€ ìˆìœ¼ë©°, {channel} ì±„ë„ì„ í†µí•´ ë°©ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ë°©ì˜ì¼ì€ {broadcast_date}, ê°œë´‰ì¼ì€ {release_date}ë¡œ ì˜ˆì •ë˜ì–´ ìˆìœ¼ë©°, ëŸ¬ë‹íƒ€ì„ì€ {running_time}ì…ë‹ˆë‹¤.
ì—°ë ¹ë“±ê¸‰ì€ {age_rating}ì´ë©°, ì˜¤ë¦¬ì§€ë„ ì—¬ë¶€ëŠ” '{is_original}'ì…ë‹ˆë‹¤.
ì‘í’ˆì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {url}

ì‘í’ˆ ìš”ì•½:
{description}
"""
            docs.append(Document(page_content=new_content, metadata=metadata))

        prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ ë¬¸ë§¥(Context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥ì— ê´€ë ¨ëœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¬¸ë§¥:
{context}

ë‹µë³€:
""")

    else:
        loader = CSVLoader(file_path=file_path, encoding="utf-8", metadata_columns=None)
        raw_docs = loader.load()
        docs = raw_docs
        print("âœ… ê¸°ë³¸ CSV ë¡œë”© ì„±ê³µ")

        prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ ë¬¸ë§¥(Context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥ì— ê´€ë ¨ëœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¬¸ë§¥:
{context}

ë‹µë³€:
""")

    return docs, prompt

@cl.on_chat_start
async def start():
    try:
        import socket
        hostname = socket.gethostname()
        ip = socket.gethostbyname(hostname)

        dataset_name = dataset_memory.get(ip, "ì¤‘ì¦ì™¸ìƒì„¼í„°_ë“±ì¥ì¸ë¬¼")
        cl.user_session.set("dataset_name", dataset_name)
        print(f"ğŸ“¦ [Chainlit] {ip} â†’ {dataset_name} ë¡œë“œ ì™„ë£Œ")

        DATA_PATH = "./data/"
        file_path = os.path.join(DATA_PATH, f"{dataset_name}.csv")
        print(f"ğŸ“ ë¶ˆëŸ¬ì˜¬ íŒŒì¼ ê²½ë¡œ: {file_path}")

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ìš”: {file_path}")

        docs, rag_prompt = process_docs(file_path)
        if not docs:
            raise ValueError("ğŸ“­ ë¬¸ì„œê°€ ë¹„ì–´ìˆì–´ìš”! CSV ë‚´ìš©ì´ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ ìˆì–´ìš”.")

        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
        split_docs = splitter.split_documents(docs)

        embedding = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(split_docs, embedding=embedding)
        retriever = vectorstore.as_retriever()

        cl.user_session.set("retriever", retriever)
        cl.user_session.set("llm", llm)
        cl.user_session.set("prompt", rag_prompt)

        print(f"âœ… ì„¸ì…˜ ì„¸íŒ… ì™„ë£Œ: {dataset_name}")

    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

@cl.on_message
async def handle_message(message: cl.Message):
    retriever = cl.user_session.get("retriever")
    llm = cl.user_session.get("llm")
    prompt = cl.user_session.get("prompt")

    user_input = message.content
    await cl.Message(content=f"â“ ì§ˆë¬¸: **{user_input}**\në¨¼ì € ë‚´ë¶€ ë¬¸ì„œì—ì„œ ì°¾ì•„ë³¼ê²Œìš”!").send()

    if retriever is None:
        await cl.Message(content="âš ï¸ retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”. ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”!").send()
        return

    internal_result = retriever.invoke(user_input)
    context = "\n".join([doc.page_content for doc in internal_result]) if internal_result else ""

    answer = None
    if context:
        prompt_text = prompt.format_prompt(context=context, question=user_input).to_string()
        answer = llm.invoke(prompt_text)

    if answer and answer.content.strip() and not _contains_not_found_phrase(answer.content):
        await cl.Message(content="âœ… ë‚´ë¶€ ë¬¸ì„œì—ì„œ ì°¾ì€ ê²°ê³¼ì…ë‹ˆë‹¤.").send()
        await cl.Message(content=answer.content).send()
    else:
        await cl.Message(content="ğŸ“­ ë‚´ë¶€ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”.\nì™¸ë¶€ ìë£Œë¥¼ ê²€ìƒ‰í•´ì„œ ì•Œë ¤ë“œë¦´ê²Œìš”!").send()
        result = summarize_search(user_input)
        await cl.Message(content=result).send()

def _contains_not_found_phrase(text: str) -> bool:
    text = text.lower()
    not_found_phrases = [
        "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
        "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤",
        "ì œê³µëœ ë¬¸ë§¥ì—ëŠ”",
        "ë‰´ìŠ¤ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì‹œê±°ë‚˜",
        "ì™¸ë¶€ì—ì„œ í™•ì¸í•´ë³´ì„¸ìš”",
        "ì •í™•í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
        "ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"
    ]
    return any(phrase in text for phrase in not_found_phrases)


from chainlit.server import app as chainlit_app

# ë¯¸ë“¤ì›¨ì–´: request.query_string ì €ì¥
class SaveQueryStringMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        query_string = request.scope.get("query_string", b"").decode()
        dataset = None
        if "dataset=" in query_string:
            dataset = unquote(query_string.split("dataset=")[-1].split("&")[0])
        if dataset:
            request.scope["dataset_name"] = dataset

            # ğŸ”¥ ì—¬ê¸°ì„œ ì „ì—­ dictì— ì €ì¥ (IP ê¸°ì¤€ or ëœë¤ ID ê¸°ì¤€ìœ¼ë¡œ)
            from starlette.middleware.base import RequestResponseEndpoint
            client_ip = request.client.host
            dataset_memory[client_ip] = dataset
            print(f"ğŸ’¾ [ë¯¸ë“¤ì›¨ì–´] {client_ip} â†’ {dataset} ì €ì¥ ì™„ë£Œ")

        response = await call_next(request)
        return response

# FastAPIì— ë¯¸ë“¤ì›¨ì–´ ë“±ë¡
chainlit_app.add_middleware(SaveQueryStringMiddleware)